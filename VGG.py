from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Flatten, Dense, Dropout


class VGG16(Model):
    def __init__(self, class_number):
        super(VGG16, self).__init__()
        self.conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')
        self.bn1 = BatchNormalization()
        self.relu1 = Activation('relu')
        self.conv2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')
        self.bn2 = BatchNormalization()
        self.relu2 = Activation('relu')
        self.max_pool1 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout1 = Dropout(0.2)

        self.conv3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')
        self.bn3 = BatchNormalization()
        self.relu3 = Activation('relu')
        self.conv4 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')
        self.bn4 = BatchNormalization()
        self.relu4 = Activation('relu')
        self.max_pool2 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout2 = Dropout(0.2)

        self.conv5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn5 = BatchNormalization()
        self.relu5 = Activation('relu')
        self.conv6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn6 = BatchNormalization()
        self.relu6 = Activation('relu')
        self.conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn7 = BatchNormalization()
        self.relu7 = Activation('relu')
        self.max_pool3 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout3 = Dropout(0.2)
        # 后两个块用的都是 512
        self.conv8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn8 = BatchNormalization()
        self.relu8 = Activation('relu')
        self.conv9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn9 = BatchNormalization()
        self.relu9 = Activation('relu')
        self.conv10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn10 = BatchNormalization()
        self.relu10 = Activation('relu')
        self.max_pool4 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout4 = Dropout(0.2)

        self.conv11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn11 = BatchNormalization()
        self.relu11 = Activation('relu')
        self.conv12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn12 = BatchNormalization()
        self.relu12 = Activation('relu')
        self.conv13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn13 = BatchNormalization()
        self.relu13 = Activation('relu')
        self.max_pool5 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout5 = Dropout(0.2)

        # 在原论文中任务做的是1000分类，全连接层采用的神经元个数分别为4096、4096、1000。
        # 在这里我的分类任务为10分类，不需要那么多神经元个数，因此改为1024、1024、10
        self.flatten = Flatten()
        self.fc1 = Dense(4096, activation='relu')
        self.d1 = Dropout(0.2)
        self.fc2 = Dense(1024, activation='relu')
        self.d2 = Dropout(0.2)
        self.FC = Dense(class_number, activation='softmax')

    def call(self, x):
        # 64通道块
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x = self.max_pool1(x)
        x = self.dropout1(x)
        # 128通道块
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu3(x)
        x = self.conv4(x)
        x = self.bn4(x)
        x = self.relu4(x)
        x = self.max_pool2(x)
        x = self.dropout2(x)
        # 256通道块
        x = self.conv5(x)
        x = self.bn5(x)
        x = self.relu5(x)
        x = self.conv6(x)
        x = self.bn6(x)
        x = self.relu6(x)
        x = self.conv7(x)
        x = self.bn7(x)
        x = self.relu7(x)
        x = self.max_pool3(x)
        x = self.dropout3(x)
        # 512通道块
        x = self.conv8(x)
        x = self.bn8(x)
        x = self.relu8(x)
        x = self.conv9(x)
        x = self.bn9(x)
        x = self.relu9(x)
        x = self.conv10(x)
        x = self.bn10(x)
        x = self.relu10(x)
        x = self.max_pool4(x)
        x = self.dropout4(x)
        # 第二个512通道块
        x = self.conv11(x)
        x = self.bn11(x)
        x = self.relu11(x)
        x = self.conv12(x)
        x = self.bn12(x)
        x = self.relu12(x)
        x = self.conv13(x)
        x = self.bn13(x)
        x = self.relu13(x)
        x = self.max_pool5(x)
        x = self.dropout5(x)
        # 全连接块
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.d1(x)
        x = self.fc2(x)
        x = self.d2(x)
        y = self.FC(x)

        return y


class VGG19(Model):
    def __init__(self, class_number):
        super(VGG19, self).__init__()
        self.conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')
        self.bn1 = BatchNormalization()
        self.relu1 = Activation('relu')
        self.conv2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')
        self.bn2 = BatchNormalization()
        self.relu2 = Activation('relu')
        self.max_pool1 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout1 = Dropout(0.2)

        self.conv3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')
        self.bn3 = BatchNormalization()
        self.relu3 = Activation('relu')
        self.conv4 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')
        self.bn4 = BatchNormalization()
        self.relu4 = Activation('relu')
        self.max_pool2 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout2 = Dropout(0.2)

        self.conv5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn5 = BatchNormalization()
        self.relu5 = Activation('relu')
        self.conv6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn6 = BatchNormalization()
        self.relu6 = Activation('relu')
        self.conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn7 = BatchNormalization()
        self.relu7 = Activation('relu')
        # 第3块在VGG16基础上新增的第4个卷积层
        self.conv3_4 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')
        self.bn3_4 = BatchNormalization()
        self.relu3_4 = Activation('relu')
        self.max_pool3 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout3 = Dropout(0.2)
        # 后两个块用的都是 512
        self.conv8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn8 = BatchNormalization()
        self.relu8 = Activation('relu')
        self.conv9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn9 = BatchNormalization()
        self.relu9 = Activation('relu')
        self.conv10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn10 = BatchNormalization()
        self.relu10 = Activation('relu')
        # 第4块新增层
        self.conv4_4 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn4_4 = BatchNormalization()
        self.relu4_4 = Activation('relu')
        self.max_pool4 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout4 = Dropout(0.2)

        self.conv11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn11 = BatchNormalization()
        self.relu11 = Activation('relu')
        self.conv12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn12 = BatchNormalization()
        self.relu12 = Activation('relu')
        self.conv13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn13 = BatchNormalization()
        self.relu13 = Activation('relu')
        # 第5块新增层
        self.conv5_4 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')
        self.bn5_4 = BatchNormalization()
        self.relu5_4 = Activation('relu')
        self.max_pool5 = MaxPool2D(pool_size=(2, 2), strides=2)
        self.dropout5 = Dropout(0.2)

        # 在原论文中任务做的是1000分类，全连接层采用的神经元个数分别为4096、4096、1000。
        # 在这里我的分类任务为10分类，不需要那么多神经元个数，因此改为1024、1024、10
        self.flatten = Flatten()
        self.fc1 = Dense(4096, activation='relu')
        self.d1 = Dropout(0.2)
        self.fc2 = Dense(1024, activation='relu')
        self.d2 = Dropout(0.2)
        self.FC = Dense(class_number, activation='softmax')

    def call(self, x):
        # 第1块  64通道
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x = self.max_pool1(x)
        x = self.dropout1(x)
        # 第2块 128通道
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu3(x)
        x = self.conv4(x)
        x = self.bn4(x)
        x = self.relu4(x)
        x = self.max_pool2(x)
        x = self.dropout2(x)
        # 第3块 256通道
        x = self.conv5(x)
        x = self.bn5(x)
        x = self.relu5(x)
        x = self.conv6(x)
        x = self.bn6(x)
        x = self.relu6(x)
        x = self.conv7(x)
        x = self.bn7(x)
        x = self.relu7(x)
        # 新增
        x = self.conv3_4(x)
        x = self.bn3_4(x)
        x = self.relu3_4(x)  # 新增
        x = self.max_pool3(x)
        x = self.dropout3(x)
        # 第4块 512通道
        x = self.conv8(x)
        x = self.bn8(x)
        x = self.relu8(x)
        x = self.conv9(x)
        x = self.bn9(x)
        x = self.relu9(x)
        x = self.conv10(x)
        x = self.bn10(x)
        x = self.relu10(x)
        # 新增
        x = self.conv4_4(x)
        x = self.bn4_4(x)
        x = self.relu4_4(x)  # 新增
        x = self.max_pool4(x)
        x = self.dropout4(x)
        # 第5块 512通道
        x = self.conv11(x)
        x = self.bn11(x)
        x = self.relu11(x)
        x = self.conv12(x)
        x = self.bn12(x)
        x = self.relu12(x)
        x = self.conv13(x)
        x = self.bn13(x)
        x = self.relu13(x)
        # 新增
        x = self.conv5_4(x)
        x = self.bn5_4(x)
        x = self.relu5_4(x)  # 新增
        x = self.max_pool5(x)
        x = self.dropout5(x)
        # 全连接层
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.d1(x)
        x = self.fc2(x)
        x = self.d2(x)
        y = self.FC(x)

        return y